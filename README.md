The multimodal Retrieval-Augmented Generation (RAG) system integrates several advanced AI technologies to provide highly accurate and contextually relevant responses. It utilizes **LLaMA 3.3 70B**, a powerful instruction-tuned model, to generate human-like responses to user queries. The system employs **Weaviate**, a vector database, for efficient management and retrieval of multimodal content. **Ollama** embeddings are used to process various content types, including text, images, audio, and tables, ensuring the system can handle diverse data formats. Additionally, **Nomic-Embed-Text** embeddings transform textual data into dense vector representations, enabling precise searches. The inclusion of **LLaVA-70B** enhances the model's ability to synthesize multimodal data, providing enriched responses based on context from different sources.




System Design of Multimodal Rag
![Rag diag drawio](https://github.com/user-attachments/assets/198ce19a-575b-486b-9fdd-19a9bd68a87d)

ScreenShot-1
![Screenshot 2024-12-27 204018](https://github.com/user-attachments/assets/841efc31-9425-4af9-b5f0-a50237938637)

Screenshot-2
![Screenshot 2024-12-27 203950](https://github.com/user-attachments/assets/4c625689-1984-441d-b55a-d93d176653b0)

Screenshot 3
![Screenshot 2024-12-27 203920](https://github.com/user-attachments/assets/ce319923-2197-415f-8183-892b335e56e4)

Screenshot-4
![Screenshot 2024-12-27 203840](https://github.com/user-attachments/assets/58bf3cb4-36d1-43e4-bc30-b36a64dff018)

ScreenShot-5
![Screenshot 2024-12-27 203714](https://github.com/user-attachments/assets/23b90488-3c80-412b-b41e-18d6ef7bc0ce)
